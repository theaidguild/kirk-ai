{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"kirk-ai","text":"<p>kirk-ai is a compact command-line interface crafted to interact with Ollama AI models. This site provides guided documentation to get you started quickly, explain the architecture, and help you integrate <code>kirk-ai</code> into your workflows.</p>"},{"location":"#quick-links","title":"Quick links","text":"<ul> <li>Installation</li> <li>Usage</li> <li>Commands</li> <li>Architecture</li> <li>Contributing</li> </ul>"},{"location":"#why-kirk-ai","title":"Why kirk-ai?","text":"<ul> <li>Minimal, focused CLI for model interactions</li> <li>Clear separation between API client, templates and commands</li> <li>Lightweight and easily extensible</li> </ul>"},{"location":"#example-quick-chat","title":"Example \u2014 quick chat","text":"<pre><code>./kirk-ai chat \"Hello \u2014 what's new?\"\n</code></pre> <p>This repository is designed for contributors and users alike. Use the sidebar to navigate deeper sections or search from the top bar.</p>"},{"location":"#project-goal-a-tpusa-specialized-ai","title":"Project goal: a TPUSA-specialized AI","text":"<p>This project aims to demonstrate a focused documentation and retrieval assistant specialized for content related to Turning Point USA (TPUSA). The site and codebase include a research dataset collected under <code>tpusa_crawl/</code> and precomputed embeddings in <code>final_embeddings.json</code> which were produced to support retrieval-augmented tasks such as search, summarization, and question-answering tailored to the collected corpus.</p> <p>Key points:</p> <ul> <li>Data provenance: the corpus used for this project comes from publicly available TPUSA pages and related materials stored under <code>tpusa_crawl/</code> in this repository. The precomputed vectors are available in <code>final_embeddings.json</code> for reproducibility and experimentation.</li> <li>Intended capabilities: the specialized AI is intended as a retrieval-augmented assistant for discovery, context-aware summarization, and example-driven code generation tied to the collected materials. It is not intended as an official TPUSA product and the repository is not affiliated with TPUSA.</li> <li>Limitations &amp; ethics: the model reflects the content of the source corpus and therefore inherits its biases and perspectives. Before using outputs in public-facing or decision-making contexts, verify claims against primary sources and consider legal and ethical constraints. Do not use the system to create targeted political persuasion; use it for research, archival, or neutral summarization tasks.</li> <li>Reproducibility: processing scripts and the data pipeline are organized under <code>tools/</code> and <code>tpusa_crawl/</code>. See the dedicated page \"TPUSA AI\" in the documentation for step-by-step notes on reproducing the dataset and embeddings.</li> </ul>"},{"location":"architecture/","title":"Architecture","text":"<p>kirk-ai is organized to keep responsibilities separated and the codebase easy to reason about.</p> <ul> <li><code>cmd/</code> \u2014 CLI command definitions and wiring (Cobra)</li> <li><code>internal/client</code> \u2014 HTTP client for Ollama interactions</li> <li><code>internal/templates</code> \u2014 Prompt templates used for code generation tasks</li> <li><code>internal/models</code> \u2014 Request/response structs</li> </ul> <p>The CLI follows a simple flow: parse flags \u2192 select model \u2192 call client \u2192 format output.</p>"},{"location":"architecture/#extending-the-cli","title":"Extending the CLI","text":"<p>Add a new command under <code>cmd/</code> and register it in <code>root.go</code>. Use existing helpers for model selection and error handling to keep behavior consistent.</p>"},{"location":"commands/","title":"Commands","text":"<p>This project uses Cobra for CLI command structure. Below are examples and common flags.</p> <p>Global flags (available to all commands): - <code>--url</code> \u2014 Ollama server URL (default: <code>http://localhost:11434</code>) - <code>--model</code> \u2014 explicitly choose a model (by default the CLI auto-selects a suitable model) - <code>-v, --verbose</code> \u2014 enable verbose output (prints metadata and progress) - <code>-s, --stream</code> \u2014 enable streaming mode where supported (prints partial model output as it arrives)</p>"},{"location":"commands/#chat","title":"chat","text":"<p>Use the <code>chat</code> command to send prompts to a selected model.</p> <p>Basic usage:</p> <pre><code>./kirk-ai chat \"Write a short poem about the sea\"\n</code></pre> <p>Advanced examples: - Stream a response and show metadata:</p> <pre><code>./kirk-ai chat \"Explain polymorphism in simple terms\" --stream --verbose\n</code></pre> <ul> <li>Force a specific model (useful when you want a known configuration):</li> </ul> <pre><code>./kirk-ai chat \"Generate unit test examples for a Go function\" --model gemma3:4b\n</code></pre> <ul> <li>Feed a long prompt from a file (shell substitution \u2014 safe for arbitrary text):</li> </ul> <pre><code>./kirk-ai chat \"$(cat my_long_prompt.txt)\" --verbose\n</code></pre> <p>Notes: - <code>chat</code> requires at least one argument (the prompt). Use shell substitution to include multi-line prompts from files. - When <code>--stream</code> is enabled the CLI prints chunks as they arrive and then a final newline; <code>--verbose</code> prints model/latency metadata.</p>"},{"location":"commands/#embed","title":"embed","text":"<p>Generate embeddings for text snippets. The <code>embed</code> command supports both single-text embeddings and embedding batches from an embeddings-ready JSON file.</p> <p>Single text example (prints embedding vector to stdout):</p> <pre><code>./kirk-ai embed \"Document text to embed\"\n</code></pre> <p>Embed chunks from a prepared JSON file (recommended when you have many documents/chunks): - Embed the first chunk (default behavior) from a file:</p> <pre><code>./kirk-ai embed --file tpusa_crawl/embeddings/tpusa_embeddings_ready.json\n</code></pre> <ul> <li>Embed all chunks from a file and write full embedding objects to disk:</li> </ul> <pre><code>./kirk-ai embed --file tpusa_crawl/embeddings/tpusa_embeddings_ready.json --all --out out/embeddings_with_vectors.json\n</code></pre> <ul> <li>Embed a specific chunk index from a file (0-based index):</li> </ul> <pre><code>./kirk-ai embed --file tpusa_crawl/embeddings/tpusa_embeddings_ready.json --chunk 42 --out out/chunk_42_embedding.json\n</code></pre> <ul> <li>Tune performance and API usage:</li> </ul> <pre><code>./kirk-ai embed --file embeddings.json --all --concurrency 8 --batch-size 20 --rate 10.0 --out embeddings-out.json\n</code></pre> <ul> <li><code>--concurrency</code> controls how many worker goroutines run in parallel</li> <li><code>--batch-size</code> controls how many chunks each worker collects before sending API calls</li> <li><code>--rate</code> sets a global requests-per-second limit (set to <code>0</code> to disable rate limiting)</li> </ul> <p>Scripting tips: - To embed many separate short texts from a file line-by-line you can combine shell tools with <code>xargs</code> or a loop:</p> <pre><code>cat texts.txt | while IFS= read -r line; do ./kirk-ai embed \"${line}\"; done\n</code></pre> <ul> <li>Use <code>--out</code> when embedding from files to get a JSON with <code>id</code>, <code>chunk_index</code>, <code>content</code>, <code>metadata</code>, and <code>embedding</code> fields which is ideal for building a vector store.</li> </ul>"},{"location":"commands/#models","title":"models","text":"<p>List models available from the Ollama server.</p> <pre><code>./kirk-ai models\n</code></pre> <p>Notes and examples: - The command prints detected capabilities (e.g., embedding, code) and a recommended model for coding and embeddings. - If no models are present the CLI will instruct you to <code>ollama pull &lt;model-name&gt;</code>.</p>"},{"location":"commands/#search","title":"search","text":"<p>Search through an embeddings file using semantic similarity.</p> <p>Basic usage:</p> <pre><code>./kirk-ai search \"How do I configure CORS?\" --embeddings out/embeddings_with_vectors.json\n</code></pre> <p>Advanced options: - Return more results and lower the similarity threshold:</p> <pre><code>./kirk-ai search \"privacy policy jurisdiction\" --embeddings embeddings.json --top-k 10 --threshold 0.55\n</code></pre> <p>Notes: - <code>--embeddings</code> is required and should point to a JSON file produced by <code>embed --out</code> (or otherwise containing <code>embedding</code> vectors). - <code>--top-k</code> and <code>--threshold</code> allow you to tune recall vs precision for your semantic search.</p>"},{"location":"commands/#rag","title":"rag","text":"<p>Retrieval-augmented generation (RAG) \u2014 answer questions using embeddings as context.</p> <p>Minimal example (requires embeddings file):</p> <pre><code>./kirk-ai rag \"What is the organization's refund policy?\" --embeddings out/embeddings_with_vectors.json\n</code></pre> <p>Useful flags and examples: - Control how many chunks are combined as context:</p> <pre><code>./kirk-ai rag \"Summarize the key benefits\" --embeddings embeddings.json --context-size 5\n</code></pre> <ul> <li>Make RAG more strict or permissive in choosing context by similarity threshold:</li> </ul> <pre><code>./kirk-ai rag \"Who is the target audience?\" --embeddings embeddings.json --similarity-threshold 0.65\n</code></pre> <ul> <li>Progressive loading for large contexts (reduces startup latency):</li> </ul> <pre><code>./kirk-ai rag \"Explain the onboarding process\" --embeddings embeddings.json --context-size 60 --progressive\n</code></pre> <ul> <li>Prefer faster (smaller) models when latency matters or override the model explicitly for RAG using <code>--rag-model</code>:</li> </ul> <pre><code>./kirk-ai rag \"Provide a short answer\" --embeddings embeddings.json --prefer-fast --rag-model gemma3:4b\n</code></pre> <p>Notes: - <code>--rag-model</code> explicitly sets the chat model used for the RAG generation step and overrides the CLI's automatic RAG model selection. The global <code>--model</code> flag is a general-purpose flag for some commands, but <code>--rag-model</code> is the recommended way to choose the chat model for <code>rag</code> to ensure the behavior you expect.</p>"},{"location":"commands/#benchmark","title":"benchmark","text":"<p>Benchmark model performance across a small set of standardized prompts.</p> <p>Basic usage (tests a chosen default or recommended coding model):</p> <pre><code>./kirk-ai benchmark\n</code></pre> <p>Advanced usage: - Test all available models:</p> <pre><code>./kirk-ai benchmark --all\n</code></pre> <ul> <li>Test a specific model (substring matching supported):</li> </ul> <pre><code>./kirk-ai benchmark --model gemma3:4b\n</code></pre> <ul> <li>Run a quicker benchmark set:</li> </ul> <pre><code>./kirk-ai benchmark --quick\n</code></pre> <p>Notes: - Benchmark prints response times and tokens/sec metrics and summarizes model reliability and speed when multiple models are tested.</p>"},{"location":"commands/#tips-troubleshooting","title":"Tips &amp; troubleshooting","text":"<ul> <li>If you see \"No models found\" errors, install a model with Ollama: <code>ollama pull &lt;model-name&gt;</code> and re-run <code>./kirk-ai models</code>.</li> <li>Use <code>--verbose</code> to get timing and progress information that helps tune concurrency, batch sizes, and rate limits.</li> <li>For automation, prefer embedding a whole dataset (<code>--file</code> + <code>--all</code>) and writing <code>--out</code> once; then run <code>search</code> or <code>rag</code> against that single canonical embeddings file.</li> <li>The default Ollama URL is <code>http://localhost:11434</code>. Set <code>--url</code> to target a remote Ollama server if needed.</li> </ul> <p>For more command-specific details, run the command with <code>--help</code> (e.g., <code>./kirk-ai embed --help</code>).</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Thanks for contributing! We welcome fixes, documentation improvements, and new features.</p>"},{"location":"contributing/#development-workflow","title":"Development workflow","text":"<ul> <li>Fork the repo and create a feature branch.</li> <li>Run <code>go fmt ./...</code> and <code>go vet ./...</code> before committing.</li> <li>Add tests alongside your code changes.</li> </ul>"},{"location":"contributing/#docs-contributions","title":"Docs contributions","text":"<ul> <li>Edit markdown files under <code>docs_src/</code>.</li> <li>Preview locally with <code>mkdocs serve -f mkdocs.yml</code>.</li> <li>Open a PR against <code>main</code> \u2014 the Pages CI pipeline will build and deploy the docs automatically.</li> </ul>"},{"location":"contributing/#reviewing","title":"Reviewing","text":"<p>Keep changes small and focused. For large changes, open an issue first to discuss the approach.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Go 1.19 or higher</li> <li>Ollama (optional) for model-based features</li> </ul>"},{"location":"installation/#quickstart","title":"Quickstart","text":"<ol> <li>Clone the repository</li> </ol> <pre><code>git clone https://github.com/theaidguild/kirk-ai.git\ncd kirk-ai\n</code></pre> <ol> <li>Download dependencies and build</li> </ol> <pre><code>go mod download &amp;&amp; go mod tidy\n\ngo build -v .\n</code></pre> <ol> <li>Run locally</li> </ol> <pre><code>./kirk-ai --help\n</code></pre> <p>For documentation preview:</p> <pre><code>pip install -r docs_requirements.txt\nmkdocs serve -f mkdocs.yml\n</code></pre>"},{"location":"tpusa_ai/","title":"TPUSA AI \u2014 Project overview","text":"<p>This page documents the project's aim to build and experiment with an AI assistant specialized for content collected from TPUSA (Turning Point USA) public materials. It explains data sources, intended uses, reproducibility notes, and ethical constraints.</p>"},{"location":"tpusa_ai/#goals","title":"Goals","text":"<ul> <li>Produce a retrieval-augmented assistant that can search, summarize, and answer questions about the TPUSA corpus.</li> <li>Provide reproducible tooling and embeddings so others can verify methods and experiment with model choices.</li> <li>Document the processing pipeline and metadata for data provenance.</li> </ul>"},{"location":"tpusa_ai/#data-sources-provenance","title":"Data sources &amp; provenance","text":"<ul> <li>Raw pages and artifacts are stored under <code>tpusa_crawl/</code> in this repository. The dataset was collected from publicly accessible pages and is included here for research and documentation purposes.</li> <li>Precomputed embeddings are stored in <code>final_embeddings.json</code>. These were produced by a pipeline that chunks documents, sanitizes HTML/text, and calls an embedding model to produce vector representations for retrieval.</li> </ul>"},{"location":"tpusa_ai/#intended-capabilities","title":"Intended capabilities","text":"<p>The specialized AI is designed for:</p> <ul> <li>Retrieval: fast, vector-based search over the corpus using the provided embeddings.</li> <li>Summarization: generate concise summaries of individual pages or clusters of pages.</li> <li>Contextual Q&amp;A: answer factual questions by retrieving the most relevant passages and generating grounded responses.</li> </ul>"},{"location":"tpusa_ai/#limitations-responsible-use","title":"Limitations &amp; responsible use","text":"<ul> <li>The assistant reflects the content and biases present in the source documents. Verify model outputs before presenting them as fact.</li> <li>Do not use the assistant to generate targeted political persuasion or microtargeted campaigning. This project is intended for research, archival, and neutral summarization and discovery tasks only.</li> <li>Respect copyright and privacy: only use or publish material you have the rights to share, and follow applicable terms of service for the sources.</li> </ul>"},{"location":"tpusa_ai/#reproducibility-pipeline","title":"Reproducibility &amp; pipeline","text":"<ul> <li>The repository contains tooling under <code>tools/processor/</code> and <code>scripts/</code> used to crawl, clean, and produce embeddings. Typical steps:</li> <li>Run the crawler to collect raw HTML (stored under <code>tpusa_crawl/raw_html/</code>).</li> <li>Run the content processor to chunk and clean text.</li> <li> <p>Produce embeddings for each chunk and store them (the resulting vectors are combined into <code>final_embeddings.json</code>).</p> </li> <li> <p>See <code>tools/processor/prepare_embeddings_data.go</code> and other helper scripts for implementation details. If you want me to add a single-command script or Make target that reproduces the pipeline, I can add it.</p> </li> </ul>"},{"location":"tpusa_ai/#demo-video","title":"Demo video","text":"Your browser does not support HTML5 video playback. You can download the demo file instead: [Download demo](../assets/media/demo.mp4)."},{"location":"tpusa_ai/#contributing","title":"Contributing","text":"<ul> <li>If you add more sources or correct metadata, include provenance (original URL, crawl date) and add tests that verify text processing edge cases.</li> <li>Keep datasets and embeddings separated from private keys and avoid committing sensitive credentials.</li> </ul>"},{"location":"tpusa_ai/#contact-disclaimers","title":"Contact &amp; disclaimers","text":"<p>This project is not affiliated with or endorsed by Turning Point USA. If you represent the rights holder for any content included here and want it removed, open an issue or submit a DMCA request following standard GitHub procedures.</p>"},{"location":"usage/","title":"Usage","text":"<p>This section explains common usage patterns for <code>kirk-ai</code>.</p>"},{"location":"usage/#list-models","title":"List models","text":"<pre><code>./kirk-ai models\n</code></pre>"},{"location":"usage/#chat","title":"Chat","text":"<pre><code>./kirk-ai chat \"Hello! Tell me a joke\"\n</code></pre>"},{"location":"usage/#embeddings","title":"Embeddings","text":"<pre><code>./kirk-ai embed \"This is some text to embed\"\n</code></pre>"},{"location":"usage/#build-and-run","title":"Build and run","text":"<pre><code>go build -v .\n./kirk-ai --help\n</code></pre>"}]}